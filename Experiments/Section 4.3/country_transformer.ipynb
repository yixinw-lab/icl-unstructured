{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50c9940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras_nlp.layers import PositionEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b9ecd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)\n",
    "random.seed(2024)\n",
    "c = pd.read_csv(\"world_population.csv\")\n",
    "c = c[['Country/Territory', 'Capital']]\n",
    "c.columns = ['country', 'capital']\n",
    "c = c[c['country'].apply(lambda x: len(x.split()) == 1)]\n",
    "c = c[c['capital'].apply(lambda x: len(x.split()) == 1)]\n",
    "c = c.reset_index(drop = True)\n",
    "c = c.sample(n=10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d751772b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries = list(c['country'])\n",
    "capitals = list(c['capital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eab508a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "middles = [\n",
    "    \"is the capital of\",\n",
    "    \"serves as capital for\",\n",
    "    \"functions as heart of\",\n",
    "    \"stands as capital of\",\n",
    "    \"operates as center for\",\n",
    "    \"represents the leadership of\",\n",
    "    \"is the nucleus of\",\n",
    "    \"acts as hub for\",\n",
    "    \"shines as capital within\",\n",
    "    \"maintains capital status in\",\n",
    "    \"anchors the government of\",\n",
    "    \"is federal capital for\",\n",
    "    \"provides the capital to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30be969b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_cc = []\n",
    "\n",
    "for i in range(len(countries)):\n",
    "    for middle in middles:\n",
    "        temp = countries[i] + ' ' + middle + ' ' + capitals[i]\n",
    "        sentences_cc.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec2bceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_c = [\n",
    "    # Brazil\n",
    "    \"Brazil's Amazon rainforest is an ecological wonder.\",\n",
    "    \"Carnival in Brazil is a spectacle of joy.\",\n",
    "    \"The biodiversity of Brazil is unmatched globally.\",\n",
    "    \n",
    "    # Dominica\n",
    "    \"Dominica is an island brimming with natural hot springs.\",\n",
    "    \"The tropical rainforests in Dominica are majestic.\",\n",
    "    \"Dominica is a haven for whale-watching enthusiasts.\",\n",
    "  \n",
    "    # Gabon\n",
    "    \"Gabon is a sanctuary for endangered forest elephants.\",\n",
    "    \"The vast rainforests of Gabon are teeming with biodiversity.\",\n",
    "    \"Gabon's efforts in conservation are globally recognized.\",\n",
    "    \n",
    "    # Liechtenstein\n",
    "    \"Liechtenstein is a haven for winter sports enthusiasts.\",\n",
    "    \"The banking sector is central to Liechtenstein's prosperity.\",\n",
    "    \"Liechtenstein boasts a variety of cultural and historical sites.\",\n",
    "    \n",
    "    # Mali\n",
    "    \"Mali's music scene has global influence and renown.\",\n",
    "    \"The legendary city of Timbuktu is located in Mali.\",\n",
    "    \"The Niger River is a lifeline for Mali's agriculture.\",\n",
    "    \n",
    "    # Mayotte\n",
    "    \"Mayotte is enveloped by a vast coral barrier reef.\",\n",
    "    \"The ylang-ylang perfume industry thrives in Mayotte.\",\n",
    "    \"Mayotte's lagoon is one of the largest in the world.\",\n",
    "    \n",
    "    # Micronesia\n",
    "    \"Micronesia is scattered across the western Pacific Ocean.\",\n",
    "    \"Traditional navigation by the stars is significant in Micronesia.\",\n",
    "    \"Micronesia has a diverse range of marine habitats.\",\n",
    "    \n",
    "    # Suriname\n",
    "    \"Suriname's rainforest is part of the Amazon basin.\",\n",
    "    \"Cultural diversity is the cornerstone of Suriname's identity.\",\n",
    "    \"Suriname is rich in biodiversity and natural resources.\",\n",
    "    \n",
    "    # Tajikistan\n",
    "    \"Tajikistan's Pamir Mountains are known as the Roof of the World.\",\n",
    "    \"Silk Road history is evident throughout Tajikistan.\",\n",
    "    \"Water resources play a crucial role in Tajikistan's agriculture.\",\n",
    "    \n",
    "    # Turkmenistan\n",
    "    \"Turkmenistan's Karakum Desert covers much of the country.\",\n",
    "    \"The ancient city of Merv in Turkmenistan is a historic jewel.\",\n",
    "    \"Turkmenistan is known for its rich reserves of natural gas.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7ea689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Every country has its unique cultural identity and heritage.\",\n",
    "    \"The national dish of a country often tells a story of its past.\",\n",
    "    \"Many countries embrace the beauty of their diverse landscapes.\",\n",
    "    \"Countries with rich histories boast numerous UNESCO World Heritage Sites.\",\n",
    "    \"Each country's flag symbolizes its distinct identity and values.\",\n",
    "    \"Countries often have traditional attire that reflects their cultural heritage.\",\n",
    "    \"National parks in various countries preserve their natural splendor.\",\n",
    "    \"A country's language is a window into its society and culture.\",\n",
    "    \"Countries around the world celebrate independence in their own unique ways.\",\n",
    "    \"The economic stability of a country affects its global influence.\",\n",
    "    \"Countries with coastlines enjoy the benefits of maritime trade.\",\n",
    "    \"Some countries are renowned for their contributions to the world of music.\",\n",
    "    \"Public transport systems can vary greatly from country to country.\",\n",
    "    \"Countries have varying forms of government, from democracies to monarchies.\",\n",
    "    \"International sporting events often bring countries together in friendly competition.\",\n",
    "    \"Folklore and legends offer intriguing insights into a country's psyche.\",\n",
    "    \"Countries prioritize education to ensure progress and development.\",\n",
    "    \"The architecture within a country can reveal its historical eras.\",\n",
    "    \"Countries with vast deserts have adapted uniquely to their environment.\",\n",
    "    \"Many countries are making efforts to combat climate change.\",\n",
    "    \"The traditional dance styles of a country are part of its allure.\",\n",
    "    \"Countries strengthen their bonds through diplomatic relations and alliances.\",\n",
    "    \"Each country deals with the challenges of urbanization in different ways.\",\n",
    "    \"Mountains serve as natural borders between some countries.\",\n",
    "    \"A country's literature often reflects its social and political issues.\",\n",
    "    \"Festivals are a colorful expression of a country's cultural fabric.\",\n",
    "    \"Countries with significant rainfall have lush, green landscapes.\",\n",
    "    \"Countries that value innovation lead in global technological advancements.\",\n",
    "    \"Traditional medicine in various countries has evolved into modern practices.\",\n",
    "    \"Countries located on tectonic plate boundaries often experience earthquakes.\",\n",
    "    \"Some countries have a vibrant street food culture that tantalizes the taste buds.\",\n",
    "    \"The currency of a country is a part of its sovereignty.\",\n",
    "    \"Tourism is a major economic driver for countries with natural wonders.\",\n",
    "    \"Countries on the equator experience a tropical climate year-round.\",\n",
    "    \"The legal system in each country has its own unique characteristics.\",\n",
    "    \"Countries with a significant youth population focus on modern education.\",\n",
    "    \"The art from different countries serves as cultural ambassadors.\",\n",
    "    \"Many countries rely on renewable energy sources for a sustainable future.\",\n",
    "    \"Topographical variety gives certain countries distinct climatic regions.\",\n",
    "    \"Countries often have national animals that symbolize their wildlife.\",\n",
    "    \"Cinema and movies are a reflection of a country's storytelling.\",\n",
    "    \"Countries with an agrarian economy focus heavily on farming.\",\n",
    "    \"Some countries experience all four seasons, while others do not.\",\n",
    "    \"Agricultural exports from various countries feed the world's population.\",\n",
    "    \"In many countries, traditional industries coexist with modern ones.\",\n",
    "    \"Countries have unique ways of celebrating life's milestones.\",\n",
    "    \"Customs and etiquette differ widely from country to country.\",\n",
    "    \"The flora and fauna of a country contribute to its biodiversity.\",\n",
    "    \"Countries facing the ocean have a rich tradition of seafaring.\",\n",
    "    \"Cuisine from different countries often includes a variety of spices.\",\n",
    "    \"Countries promote their language and culture through educational exchanges.\",\n",
    "    \"The spiritual life in countries can vary widely among the population.\",\n",
    "    \"Each country has a history marked by significant events and epochs.\",\n",
    "    \"The national anthems of countries evoke patriotism and unity.\",\n",
    "    \"Countries with rivers often develop rich agricultural and cultural societies.\",\n",
    "    \"Local markets in countries are melting pots of tradition and trade.\",\n",
    "    \"Startups and entrepreneurship are thriving in various forward-thinking countries.\",\n",
    "    \"Fashion trends in different countries can influence global styles.\",\n",
    "    \"Countries implement measures to protect their wildlife and ecosystems.\",\n",
    "    \"In many countries, handcrafting skills are passed down through generations.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf007123",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_cccc = []\n",
    "\n",
    "for i in range(len(sentences_cc)):\n",
    "    for j in range(len(sentences_cc)):\n",
    "        if i != j and i // 13 != j // 13:\n",
    "            sentences_cccc.append(sentences_cc[i] + ' ' + sentences_cc[j])\n",
    "            \n",
    "sentences_cccc = random.sample(sentences_cccc, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11b544e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = sentences_cc + sentences_c + sentences + sentences_cccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6352812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71751f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brazil functions as heart of Brasilia Turkmenistan operates as center for Ashgabat'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_cccc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a8a86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = []\n",
    "\n",
    "def clean_text(x):\n",
    "    temp = x.lower()\n",
    "    if temp.endswith(\"'s\"):\n",
    "        temp = temp[:-2]\n",
    "    return temp\n",
    "    \n",
    "for sentence in all_sentences:\n",
    "    tokens = [clean_text(x) for x in sentence.split()]\n",
    "    cleaned_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16046733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = set([item for sublist in cleaned_sentences for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "415b5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {}\n",
    "\n",
    "cnt = 1\n",
    "\n",
    "for x in c['country'].str.lower():\n",
    "    vocab_map[x] = cnt\n",
    "    cnt += 1\n",
    "    \n",
    "for x in c['capital'].str.lower():\n",
    "    vocab_map[x] = cnt\n",
    "    cnt += 1\n",
    "    \n",
    "for x in vocab:\n",
    "    if x not in vocab_map:\n",
    "        vocab_map[x] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30b76691",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences_number = []\n",
    "\n",
    "for sentence in cleaned_sentences:  \n",
    "    temp = []\n",
    "    for word in sentence:\n",
    "        temp.append(vocab_map[word])\n",
    "    \n",
    "    cleaned_sentences_number.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dfdf5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_len = max(len(sentence) for sentence in cleaned_sentences_number)\n",
    "x_train_padded = pad_sequences(cleaned_sentences_number, maxlen=max_sequence_len, padding='post')\n",
    "\n",
    "n_cat = len(vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87530315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 357, 205, 389, 161,  11,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c51b7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train_padded)\n",
    "n_cat = len(vocab_map)\n",
    "np.random.shuffle(x_train)\n",
    "x_masked_train = np.copy(x_train)\n",
    "x_masked_train = x_masked_train[:,:-1]\n",
    "y_masked_labels_train = np.copy(x_train)\n",
    "y_masked_labels_train = y_masked_labels_train[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "816be2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "embed_dim = 100\n",
    "num_heads = 2\n",
    "num_blocks = 5\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "input_layer = layers.Input(shape=(x_masked_train.shape[1],), dtype=tf.int32)  # Input layer\n",
    "\n",
    "embedding_layer = layers.Embedding(n_cat + 1, embed_dim, name=\"word_embedding\")(input_layer)  # Embedding layer\n",
    "position_embeddings = PositionEmbedding(sequence_length=len(x_masked_train[0]))(embedding_layer)\n",
    "embedding_layer = embedding_layer + position_embeddings\n",
    "\n",
    "# Transformer blocks with causal masking for next token prediction\n",
    "x = embedding_layer\n",
    "for i in range(num_blocks):\n",
    "    # Apply the causal mask to ensure that each position can only attend to known tokens\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads\n",
    "    )(x, x, x, use_causal_mask=True)\n",
    "    \n",
    "    x = layers.Add()([x, attention_output])  # Skip Connection\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    ff_net = keras.models.Sequential([\n",
    "        layers.Dense(2 * embed_dim, activation='relu'),\n",
    "        layers.Dense(embed_dim),\n",
    "    ])\n",
    "\n",
    "    # Apply Feedforward network\n",
    "    x = ff_net(x)\n",
    "\n",
    "    # Add & Normalize\n",
    "    x = layers.Add()([attention_output, x]) \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "# Output layer for providing predictions over the vocabulary\n",
    "predict_layer = layers.Dense(n_cat, activation='softmax')(x)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=predict_layer)  # Model definition\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'],\n",
    "             weighted_metrics=[])  # Compile the model\n",
    "\n",
    "# Reshape the target data to have an extra dimension\n",
    "y_masked_labels_train_reshaped = y_masked_labels_train.reshape(y_masked_labels_train.shape[0], \n",
    "                                                               y_masked_labels_train.shape[1], 1)\n",
    "\n",
    "target_mask = np.where(y_masked_labels_train_reshaped == 0, 0, 1)\n",
    "\n",
    "y_masked_labels_train -= 1\n",
    "y_masked_labels_train[y_masked_labels_train < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ce13ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 13s 13s/step - loss: 5.2556 - accuracy: 0.0000e+00 - val_loss: 4.7134 - val_accuracy: 0.0564\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 4.6434 - accuracy: 0.0519 - val_loss: 4.3751 - val_accuracy: 0.1262\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 4.3120 - accuracy: 0.1254 - val_loss: 4.1495 - val_accuracy: 0.1262\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 4.0903 - accuracy: 0.1254 - val_loss: 3.9627 - val_accuracy: 0.1883\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 3.9042 - accuracy: 0.1866 - val_loss: 3.7892 - val_accuracy: 0.2165\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 3.7342 - accuracy: 0.2164 - val_loss: 3.6414 - val_accuracy: 0.2515\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 3.5910 - accuracy: 0.2481 - val_loss: 3.5038 - val_accuracy: 0.2383\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 3.4582 - accuracy: 0.2358 - val_loss: 3.3716 - val_accuracy: 0.2447\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 3.3318 - accuracy: 0.2434 - val_loss: 3.2571 - val_accuracy: 0.2512\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 3.2231 - accuracy: 0.2507 - val_loss: 3.1460 - val_accuracy: 0.2497\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 3.1169 - accuracy: 0.2489 - val_loss: 3.0422 - val_accuracy: 0.2587\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 3.0174 - accuracy: 0.2556 - val_loss: 2.9478 - val_accuracy: 0.2620\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 2.9266 - accuracy: 0.2585 - val_loss: 2.8607 - val_accuracy: 0.2622\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.8426 - accuracy: 0.2597 - val_loss: 2.7798 - val_accuracy: 0.2579\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.7645 - accuracy: 0.2592 - val_loss: 2.7040 - val_accuracy: 0.2645\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.6911 - accuracy: 0.2645 - val_loss: 2.6354 - val_accuracy: 0.2794\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.6245 - accuracy: 0.2787 - val_loss: 2.5676 - val_accuracy: 0.3053\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.5591 - accuracy: 0.3036 - val_loss: 2.5011 - val_accuracy: 0.3240\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2.4947 - accuracy: 0.3220 - val_loss: 2.4371 - val_accuracy: 0.3276\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.4331 - accuracy: 0.3258 - val_loss: 2.3742 - val_accuracy: 0.3260\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 2.3721 - accuracy: 0.3247 - val_loss: 2.3119 - val_accuracy: 0.3337\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.3116 - accuracy: 0.3287 - val_loss: 2.2510 - val_accuracy: 0.3413\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 2.2527 - accuracy: 0.3352 - val_loss: 2.1927 - val_accuracy: 0.3462\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.1959 - accuracy: 0.3426 - val_loss: 2.1376 - val_accuracy: 0.3460\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.1421 - accuracy: 0.3426 - val_loss: 2.0850 - val_accuracy: 0.3561\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2.0908 - accuracy: 0.3526 - val_loss: 2.0376 - val_accuracy: 0.3579\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.0436 - accuracy: 0.3555 - val_loss: 1.9930 - val_accuracy: 0.3663\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.0005 - accuracy: 0.3600 - val_loss: 1.9586 - val_accuracy: 0.3623\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.9639 - accuracy: 0.3616 - val_loss: 1.9233 - val_accuracy: 0.3598\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.9329 - accuracy: 0.3556 - val_loss: 1.8886 - val_accuracy: 0.3615\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.8924 - accuracy: 0.3623 - val_loss: 1.8499 - val_accuracy: 0.3713\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.8537 - accuracy: 0.3720 - val_loss: 1.8216 - val_accuracy: 0.3672\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.8281 - accuracy: 0.3635 - val_loss: 1.7960 - val_accuracy: 0.3758\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.7995 - accuracy: 0.3739 - val_loss: 1.7672 - val_accuracy: 0.3824\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.7694 - accuracy: 0.3775 - val_loss: 1.7412 - val_accuracy: 0.3862\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.7432 - accuracy: 0.3824 - val_loss: 1.7215 - val_accuracy: 0.3736\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.7219 - accuracy: 0.3738 - val_loss: 1.6989 - val_accuracy: 0.3933\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.6989 - accuracy: 0.3939 - val_loss: 1.6733 - val_accuracy: 0.3824\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.6710 - accuracy: 0.3839 - val_loss: 1.6514 - val_accuracy: 0.3944\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.6474 - accuracy: 0.3984 - val_loss: 1.6325 - val_accuracy: 0.4012\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1.6252 - accuracy: 0.4029 - val_loss: 1.6141 - val_accuracy: 0.3880\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.6071 - accuracy: 0.3910 - val_loss: 1.6125 - val_accuracy: 0.4003\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.6005 - accuracy: 0.4056 - val_loss: 1.6330 - val_accuracy: 0.3809\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.6204 - accuracy: 0.3844 - val_loss: 1.6009 - val_accuracy: 0.3932\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1.5865 - accuracy: 0.3947 - val_loss: 1.6045 - val_accuracy: 0.4230\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.5822 - accuracy: 0.4276 - val_loss: 1.5650 - val_accuracy: 0.4053\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1.5432 - accuracy: 0.4105 - val_loss: 1.5662 - val_accuracy: 0.3945\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.5447 - accuracy: 0.3981 - val_loss: 1.5324 - val_accuracy: 0.4142\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.5051 - accuracy: 0.4212 - val_loss: 1.5294 - val_accuracy: 0.4135\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.4961 - accuracy: 0.4197 - val_loss: 1.5176 - val_accuracy: 0.4250\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.4789 - accuracy: 0.4254 - val_loss: 1.4933 - val_accuracy: 0.4221\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1.4527 - accuracy: 0.4243 - val_loss: 1.4774 - val_accuracy: 0.4204\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 1.4372 - accuracy: 0.4283 - val_loss: 1.4550 - val_accuracy: 0.4398\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.4115 - accuracy: 0.4456 - val_loss: 1.4431 - val_accuracy: 0.4503\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.3968 - accuracy: 0.4561 - val_loss: 1.4282 - val_accuracy: 0.4615\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.3785 - accuracy: 0.4645 - val_loss: 1.4137 - val_accuracy: 0.4546\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1.3557 - accuracy: 0.4600 - val_loss: 1.3975 - val_accuracy: 0.4624\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.3353 - accuracy: 0.4676 - val_loss: 1.3831 - val_accuracy: 0.4635\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 1.3196 - accuracy: 0.4738 - val_loss: 1.3744 - val_accuracy: 0.4648\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.3043 - accuracy: 0.4633 - val_loss: 1.3567 - val_accuracy: 0.4848\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.2877 - accuracy: 0.4914 - val_loss: 1.3456 - val_accuracy: 0.4766\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.2690 - accuracy: 0.4840 - val_loss: 1.3279 - val_accuracy: 0.4888\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.2497 - accuracy: 0.4977 - val_loss: 1.3140 - val_accuracy: 0.4932\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.2334 - accuracy: 0.4980 - val_loss: 1.3601 - val_accuracy: 0.4762\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.2779 - accuracy: 0.4761 - val_loss: 1.3079 - val_accuracy: 0.4891\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.2236 - accuracy: 0.4951 - val_loss: 1.3057 - val_accuracy: 0.5123\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 1.2218 - accuracy: 0.5160 - val_loss: 1.2853 - val_accuracy: 0.5003\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.1929 - accuracy: 0.5060 - val_loss: 1.2671 - val_accuracy: 0.5216\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.1747 - accuracy: 0.5292 - val_loss: 1.2385 - val_accuracy: 0.5418\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.1509 - accuracy: 0.5466 - val_loss: 1.2263 - val_accuracy: 0.5471\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.1378 - accuracy: 0.5492 - val_loss: 1.2336 - val_accuracy: 0.5373\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1.1270 - accuracy: 0.5350 - val_loss: 1.1917 - val_accuracy: 0.5663\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.0934 - accuracy: 0.5667 - val_loss: 1.1793 - val_accuracy: 0.5686\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 1.0832 - accuracy: 0.5723 - val_loss: 1.1750 - val_accuracy: 0.5425\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.0727 - accuracy: 0.5370 - val_loss: 1.1348 - val_accuracy: 0.5814\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.0304 - accuracy: 0.5788 - val_loss: 1.1250 - val_accuracy: 0.5791\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.0220 - accuracy: 0.5840 - val_loss: 1.1010 - val_accuracy: 0.5995\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.9900 - accuracy: 0.6036 - val_loss: 1.0848 - val_accuracy: 0.6036\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.9733 - accuracy: 0.6030 - val_loss: 1.0581 - val_accuracy: 0.5984\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.9456 - accuracy: 0.5999 - val_loss: 1.0422 - val_accuracy: 0.5947\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.9303 - accuracy: 0.6000 - val_loss: 1.0182 - val_accuracy: 0.6113\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.9013 - accuracy: 0.6161 - val_loss: 1.0083 - val_accuracy: 0.6135\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.8850 - accuracy: 0.6171 - val_loss: 0.9988 - val_accuracy: 0.6168\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.8722 - accuracy: 0.6194 - val_loss: 0.9746 - val_accuracy: 0.6161\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.8482 - accuracy: 0.6208 - val_loss: 0.9636 - val_accuracy: 0.6150\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.8373 - accuracy: 0.6227 - val_loss: 0.9546 - val_accuracy: 0.6164\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.8220 - accuracy: 0.6209 - val_loss: 0.9481 - val_accuracy: 0.6161\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.8093 - accuracy: 0.6221 - val_loss: 0.9385 - val_accuracy: 0.6167\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.7973 - accuracy: 0.6243 - val_loss: 0.9276 - val_accuracy: 0.6156\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.7852 - accuracy: 0.6230 - val_loss: 0.9238 - val_accuracy: 0.6161\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.7770 - accuracy: 0.6277 - val_loss: 0.9177 - val_accuracy: 0.6178\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.7650 - accuracy: 0.6305 - val_loss: 0.9147 - val_accuracy: 0.6165\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.7575 - accuracy: 0.6294 - val_loss: 0.9118 - val_accuracy: 0.6152\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.7507 - accuracy: 0.6235 - val_loss: 0.9068 - val_accuracy: 0.6134\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.7420 - accuracy: 0.6264 - val_loss: 0.9039 - val_accuracy: 0.6128\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.7349 - accuracy: 0.6295 - val_loss: 0.9021 - val_accuracy: 0.6141\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.7286 - accuracy: 0.6331 - val_loss: 0.8989 - val_accuracy: 0.6161\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.7216 - accuracy: 0.6318 - val_loss: 0.8964 - val_accuracy: 0.6153\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.7161 - accuracy: 0.6296 - val_loss: 0.8949 - val_accuracy: 0.6133\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.7101 - accuracy: 0.6309 - val_loss: 0.8949 - val_accuracy: 0.6130\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.7047 - accuracy: 0.6340 - val_loss: 0.8938 - val_accuracy: 0.6131\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.6988 - accuracy: 0.6357 - val_loss: 0.8923 - val_accuracy: 0.6135\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.6935 - accuracy: 0.6352 - val_loss: 0.8920 - val_accuracy: 0.6139\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.6887 - accuracy: 0.6366 - val_loss: 0.8922 - val_accuracy: 0.6131\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.6833 - accuracy: 0.6384 - val_loss: 0.8918 - val_accuracy: 0.6131\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.6786 - accuracy: 0.6393 - val_loss: 0.8903 - val_accuracy: 0.6116\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.6737 - accuracy: 0.6400 - val_loss: 0.8898 - val_accuracy: 0.6115\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.6689 - accuracy: 0.6421 - val_loss: 0.8904 - val_accuracy: 0.6128\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.6644 - accuracy: 0.6440 - val_loss: 0.8908 - val_accuracy: 0.6124\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.6601 - accuracy: 0.6455 - val_loss: 0.8902 - val_accuracy: 0.6117\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.6556 - accuracy: 0.6458 - val_loss: 0.8900 - val_accuracy: 0.6112\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.6514 - accuracy: 0.6473 - val_loss: 0.8903 - val_accuracy: 0.6123\n"
     ]
    }
   ],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(x_masked_train, y_masked_labels_train_reshaped, \n",
    "                    sample_weight=target_mask, epochs=1000, batch_size=batch_size,\n",
    "                    validation_split = 0.5, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9483d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)\n",
    "random.seed(2024)\n",
    "\n",
    "def get_accuracy(num_ex): ### num_ex = 1 to 5\n",
    "    acc_a = []\n",
    "    samples = [random.sample([x for x in range(1,11)], num_ex + 1) for _ in range(500)]\n",
    "    for sample in samples:\n",
    "        temp = sample[:-1]\n",
    "        targ = sample[-1]\n",
    "        temp_a = []\n",
    "        for x in temp:\n",
    "            temp_a += [x, x + c.shape[0]]\n",
    "        temp_a += [targ]\n",
    "        targ_a = targ + c.shape[0]\n",
    "        temp_a += [0] * (len(x_masked_train[0]) - len(temp_a))\n",
    "        pred_a = keras.backend.function(inputs = model.layers[0].input, outputs = model.layers[-1].output) \\\n",
    "                (np.array(temp_a).reshape(1,len(temp_a)))\n",
    "        if pred_a[:,(2 * num_ex),:][:(2 * c.shape[0])].argmax() + 1 == targ_a:\n",
    "            acc_a.append(1)\n",
    "    return np.sum(acc_a)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87e4e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(1))\n",
    "print(get_accuracy(2))\n",
    "print(get_accuracy(3))\n",
    "print(get_accuracy(4))\n",
    "print(get_accuracy(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
